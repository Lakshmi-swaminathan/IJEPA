{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "# Transform: Resize to 224x224 and convert to Tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to 224x224 for VGG-16\n",
    "    transforms.ToTensor(),  # Convert to Tensor\n",
    "])\n",
    "\n",
    "# Load CIFAR-100 training dataset\n",
    "cifar100_train = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Create a DataLoader for CIFAR-100 training data\n",
    "dataloader = DataLoader(cifar100_train, batch_size=512, shuffle=True, num_workers=4)\n",
    "\n",
    "# Explanation:\n",
    "# - batch_size=32: Loads 32 images per batch.\n",
    "# - shuffle=True: Shuffles the dataset at every epoch to improve training.\n",
    "# - num_workers=4: Uses 4 subprocesses to load the data in parallel (improves performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vgg16\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Load VGG-16 model pretrained on ImageNet\n",
    "        vgg = vgg16(pretrained=pretrained)\n",
    "        \n",
    "        # Use only the convolutional layers (we don't need the fully connected layers)\n",
    "        self.features = vgg.features  # This will contain all the conv layers\n",
    "        \n",
    "        # Optional: You could freeze some layers if you want to fine-tune only part of the network.\n",
    "        # For example, to freeze the first few layers:\n",
    "        # for param in self.features[:10].parameters():\n",
    "        #     param.requires_grad = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass to extract features from the image.\n",
    "        \n",
    "        Input:\n",
    "        - x: The input image tensor (B, C, H, W) where:\n",
    "          B is the batch size\n",
    "          C is the number of channels (3 for RGB images)\n",
    "          H and W are the height and width of the image.\n",
    "        \n",
    "        Output:\n",
    "        - features: The extracted feature map (B, 512, H_out, W_out), where:\n",
    "          512 is the number of output channels from the final VGG-16 layer\n",
    "          H_out and W_out are the spatial dimensions of the feature map (usually 7x7 for 224x224 input).\n",
    "        \"\"\"\n",
    "        features = self.features(x)  # Pass through the convolutional layers\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self, input_dim=512, hidden_dim=4096, output_dim=512):\n",
    "        super(Predictor, self).__init__()\n",
    "        \n",
    "        # Linear layers to predict target features from context features\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)  # Input to hidden layer\n",
    "        self.relu = nn.ReLU()                       # Non-linearity\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim) # Hidden to output layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass through the predictor\n",
    "        x = self.fc1(x)         # Input -> Hidden layer\n",
    "        x = self.relu(x)        # Non-linearity\n",
    "        x = self.fc2(x)         # Hidden layer -> Output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA:\n",
    "    def __init__(self, model, decay=0.99):\n",
    "        \"\"\"\n",
    "        Initialize EMA with a given model and decay rate.\n",
    "        \n",
    "        Args:\n",
    "        - model: The model to track with EMA (usually the target encoder).\n",
    "        - decay: The decay rate for updating EMA (default: 0.99).\n",
    "        \"\"\"\n",
    "        self.model = model  # The target encoder\n",
    "        self.decay = decay  # EMA decay factor (typically close to 1, e.g., 0.99)\n",
    "        self.shadow = {}    # Stores the moving average of the parameters\n",
    "        self.backup = {}    # Temporary backup of model parameters during shadow application\n",
    "\n",
    "    def register(self):\n",
    "        \"\"\"Initialize the shadow weights with the original model weights.\"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"Update the shadow weights using the current model parameters with EMA.\"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                # EMA update: shadow[name] = decay * shadow[name] + (1 - decay) * param\n",
    "                self.shadow[name] = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n",
    "\n",
    "    def apply_shadow(self):\n",
    "        \"\"\"Replace the model parameters with the EMA weights (shadow weights).\"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                # Backup current parameters\n",
    "                self.backup[name] = param.data.clone()\n",
    "                # Replace parameters with EMA weights\n",
    "                param.data = self.shadow[name]\n",
    "\n",
    "    def restore(self):\n",
    "        \"\"\"Restore the original model parameters after using the shadow weights.\"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                # Restore from backup\n",
    "                param.data = self.backup[name]\n",
    "        # Clear the backup\n",
    "        self.backup = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Positional Embedding Layer for the patches\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, grid_size, embed_dim):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.grid_size = grid_size  # E.g., 7 for a 7x7 grid (49 patches total)\n",
    "        self.embed_dim = embed_dim  # Embedding dimension (e.g., 512 to match VGG-16 output)\n",
    "        \n",
    "        # Learnable positional embeddings\n",
    "        self.positional_embeddings = nn.Parameter(torch.randn(grid_size * grid_size, embed_dim))\n",
    "\n",
    "    def forward(self):\n",
    "        return self.positional_embeddings  # Shape: (grid_size * grid_size, embed_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_blocks_without_positional_embeddings(image, context_encoder, M=4, N=49, context_ratio=0.85, device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Generate context and target blocks without positional embeddings.\n",
    "    \"\"\"\n",
    "    _, H, W = image.shape  # Assume image in [C, H, W] format, with C=3\n",
    "    grid_size = int(np.sqrt(N))\n",
    "    patch_h, patch_w = H // grid_size, W // grid_size\n",
    "\n",
    "    # Step 1: Split the image into patches\n",
    "    patches = []\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            patch = image[:, i*patch_h:(i+1)*patch_h, j*patch_w:(j+1)*patch_w]  # (3, patch_h, patch_w)\n",
    "            patches.append(patch.to(device))\n",
    "\n",
    "    # Select context and target patches\n",
    "    num_context_patches = int(context_ratio * N)\n",
    "    context_indices = np.random.choice(range(N), size=num_context_patches, replace=False)\n",
    "    target_indices = [i for i in range(N) if i not in context_indices]\n",
    "\n",
    "    # Stack patches for encoder compatibility\n",
    "    context_patches = torch.stack([patches[i] for i in context_indices], dim=0)  # Shape: [num_context_patches, 3, patch_h, patch_w]\n",
    "    target_patches = torch.stack([patches[i] for i in target_indices[:M]], dim=0)  # Shape: [M, 3, patch_h, patch_w]\n",
    "\n",
    "    return context_patches, target_patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhishek/MSCS/cs59000-deep-learning/I-JEPA_Re-Implementation/.venv/lib64/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/abhishek/MSCS/cs59000-deep-learning/I-JEPA_Re-Implementation/.venv/lib64/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Initialize encoders\n",
    "context_encoder = Encoder().to(device)\n",
    "target_encoder = Encoder().to(device)  # Target encoder uses EMA of context encoder's weights\n",
    "predictor = Predictor().to(device)\n",
    "\n",
    "# Initialize EMA\n",
    "ema = EMA(target_encoder, decay=0.99)\n",
    "ema.register()  # Register initial weights\n",
    "\n",
    "# Optimizer for context encoder and predictor\n",
    "optimizer = optim.Adam(list(context_encoder.parameters()) + list(predictor.parameters()), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Backpropagation and update weights\u001b[39;00m\n\u001b[1;32m     39\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 40\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Update EMA for target encoder\u001b[39;00m\n",
      "File \u001b[0;32m~/MSCS/cs59000-deep-learning/I-JEPA_Re-Implementation/.venv/lib64/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MSCS/cs59000-deep-learning/I-JEPA_Re-Implementation/.venv/lib64/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MSCS/cs59000-deep-learning/I-JEPA_Re-Implementation/.venv/lib64/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Training loop with global average pooling and progress tracking\n",
    "for epoch in range(10):\n",
    "    epoch_loss = 0.0\n",
    "    print(f\"Epoch [{epoch + 1}/10]\")\n",
    "\n",
    "    # Progress bar for tracking each batch\n",
    "    for batch_idx, (images, _) in enumerate(tqdm(dataloader, desc=\"Training\", leave=False)):\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Generate context and target blocks without positional embeddings\n",
    "        context_patches, target_patches = generate_blocks_without_positional_embeddings(\n",
    "            images[0], context_encoder=context_encoder, M=4, N=49, device=device\n",
    "        )\n",
    "\n",
    "        # Forward pass through context encoder\n",
    "        context_features_all = torch.stack([context_encoder(patch.unsqueeze(0)).squeeze(0) for patch in context_patches])\n",
    "        context_features_all = F.adaptive_avg_pool2d(context_features_all, (1, 1)).squeeze(-1).squeeze(-1)  # Shape: [batch_size, 512]\n",
    "\n",
    "        # Select only the first M context features to match the number of target features\n",
    "        context_features = context_features_all[:len(target_patches)]\n",
    "\n",
    "        # Apply shadow (EMA weights) to the target encoder for this step\n",
    "        ema.apply_shadow()  # Use EMA weights for target encoder\n",
    "        target_features = torch.stack([target_encoder(patch.unsqueeze(0)).squeeze(0) for patch in target_patches])\n",
    "        target_features = F.adaptive_avg_pool2d(target_features, (1, 1)).squeeze(-1).squeeze(-1)  # Shape: [batch_size, 512]\n",
    "        ema.restore()       # Restore original weights after prediction\n",
    "\n",
    "        # Predict target features using predictor\n",
    "        predicted_target_features = predictor(context_features)\n",
    "\n",
    "        # Compute loss (MSE between predicted and actual target features)\n",
    "        loss = nn.MSELoss()(predicted_target_features, target_features)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Backpropagation and update weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update EMA for target encoder\n",
    "        ema.update()\n",
    "\n",
    "        # Print batch loss periodically (every 10 batches)\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/10], Batch [{batch_idx}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch + 1}/10] Average Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "    # Optional: Save checkpoint every few epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        torch.save(context_encoder.state_dict(), f\"context_encoder_epoch_{epoch + 1}.pth\")\n",
    "        torch.save(target_encoder.state_dict(), f\"target_encoder_epoch_{epoch + 1}.pth\")\n",
    "        torch.save(predictor.state_dict(), f\"predictor_epoch_{epoch + 1}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
