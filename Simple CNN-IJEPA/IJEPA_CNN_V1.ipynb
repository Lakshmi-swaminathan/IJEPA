{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBOQDP93diZb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from google.colab import drive\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFhJzJ5DlB-o",
        "outputId": "72add21a-b61e-4503-9cf7-5423766ec737"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CNN architecture used for encoding target/context blocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaDY7wI_wCui"
      },
      "outputs": [],
      "source": [
        "# Define a simple CNN encoder for context and target encoders\n",
        "class CNNEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim=512):\n",
        "        super(CNNEncoder, self).__init__()\n",
        "        # A simple CNN with a few convolutional layers followed by a fully connected layer\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),  # (B, 3, 224, 224) -> (B, 32, 224, 224)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # (B, 32, 224, 224) -> (B, 32, 112, 112)\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # (B, 32, 112, 112) -> (B, 64, 112, 112)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # (B, 64, 112, 112) -> (B, 64, 56, 56)\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # (B, 64, 56, 56) -> (B, 128, 56, 56)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # (B, 128, 56, 56) -> (B, 128, 28, 28)\n",
        "        )\n",
        "        self.fc = nn.Linear(128 * 28 * 28, embed_dim)  # Fully connected layer for embedding\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)  # Pass through convolutional layers\n",
        "        x = x.view(x.size(0), -1)  # Flatten the feature map\n",
        "        return self.fc(x)  # Output embedding\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEYVoB7TKbI1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# The context encoder and target encoder will be the same\n",
        "class ContextEncoder(CNNEncoder):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmSTl20iKdmI"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class TargetEncoder(CNNEncoder):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The predictor used to predict the target blocks based on context blocks as input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbVy30K4Kfg6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the predictor\n",
        "class Predictor(nn.Module):\n",
        "    def __init__(self, embed_dim=512):\n",
        "        super(Predictor, self).__init__()\n",
        "        self.fc1 = nn.Linear(embed_dim, embed_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        return self.fc2(x)  # Predict the target representation from context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDi8YkaCwQ8b"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Self-supervised loss (L2 Loss)\n",
        "class IJEPALoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(IJEPALoss, self).__init__()\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "    def forward(self, pred_target_rep, actual_target_rep):\n",
        "        return self.criterion(pred_target_rep, actual_target_rep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVd4i2FiwVgG"
      },
      "outputs": [],
      "source": [
        "# Function to create context and target patches and resize them to (224, 224)\n",
        "def create_context_target_blocks(image, mask_ratio=0.25):\n",
        "    B, C, H, W = image.shape\n",
        "    mask_size = int(H * mask_ratio)  # Calculate mask size based on mask ratio\n",
        "\n",
        "    # Separate context and target blocks\n",
        "    context = image[:, :, :H - mask_size, :W - mask_size]\n",
        "    target = image[:, :, mask_size:, mask_size:]\n",
        "\n",
        "    # Resize context and target back to (224, 224)\n",
        "    resize_transform = transforms.Resize((224, 224))\n",
        "    context = resize_transform(context)\n",
        "    target = resize_transform(target)\n",
        "\n",
        "    return context, target\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I-JEPA Pretraining function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chFclihMEV6c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# I-JEPA Pretraining function \n",
        "def pretrain_ijepa(context_encoder, target_encoder, predictor, train_loader, start_epoch=56, epochs=200, lr=0.001, patience=3):\n",
        "    optimizer = optim.Adam(list(context_encoder.parameters()) + list(predictor.parameters()), lr=lr)\n",
        "    criterion = IJEPALoss()\n",
        "\n",
        "    context_encoder.train()\n",
        "    target_encoder.train()\n",
        "    predictor.train()\n",
        "\n",
        "    best_loss = 2.754716042266495e-05\n",
        "    patience_counter = 0\n",
        "    best_model_path = '/content/drive/MyDrive/best_ijepa_model_CNN_final.pth'\n",
        "\n",
        "    # If starting from a specific epoch, load the saved state\n",
        "    if start_epoch > 0:\n",
        "        checkpoint = torch.load(best_model_path, map_location=torch.device('cpu'))\n",
        "        context_encoder.load_state_dict(checkpoint['context_encoder'])\n",
        "        target_encoder.load_state_dict(checkpoint['target_encoder'])\n",
        "        predictor.load_state_dict(checkpoint['predictor'])\n",
        "        print(f\"Resumed training from epoch {start_epoch}\")\n",
        "\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "        running_loss = 0.0\n",
        "        for images, _ in train_loader:\n",
        "            images = images.to(device)\n",
        "\n",
        "            # Create context and target blocks\n",
        "            context, target = create_context_target_blocks(images)\n",
        "            context, target = context.to(device), target.to(device)\n",
        "\n",
        "            # Get representations\n",
        "            context_rep = context_encoder(context)\n",
        "            actual_target_rep = target_encoder(target)\n",
        "            pred_target_rep = predictor(context_rep)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(pred_target_rep, actual_target_rep)\n",
        "\n",
        "            # Optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {avg_loss}')\n",
        "\n",
        "        # Check for early stopping\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            patience_counter = 0\n",
        "            torch.save({\n",
        "                'context_encoder': context_encoder.state_dict(),\n",
        "                'target_encoder': target_encoder.state_dict(),\n",
        "                'predictor': predictor.state_dict(),\n",
        "            }, best_model_path)\n",
        "            print(f\"Best model saved at epoch {epoch + 1} with loss {best_loss}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"Patience counter: {patience_counter}\")\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping triggered!\")\n",
        "            break\n",
        "\n",
        "    print(\"I-JEPA pretraining complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGwqEHuxzZ4F",
        "outputId": "4c19d37a-4105-4847-db4d-f76c115d2489"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:14<00:00, 12073587.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        }
      ],
      "source": [
        "# DataLoader for CIFAR-10\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resizing for CNN\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "valset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "val_loader = DataLoader(valset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugMLOPhjw6q3"
      },
      "outputs": [],
      "source": [
        "# Initialize the context encoder, target encoder, and predictor\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "context_encoder = ContextEncoder().to(device)\n",
        "target_encoder = TargetEncoder().to(device)\n",
        "predictor = Predictor().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EK4VR7lSw9Sa",
        "outputId": "bd07c680-9fea-4143-8252-4fff1b1a5253"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-6-d36521526031>:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(best_model_path, map_location=torch.device('cpu'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resumed training from epoch 56\n",
            "Epoch 57/200, Loss: 2.8042292487813527e-05\n",
            "Patience counter: 1\n",
            "Epoch 58/200, Loss: 2.6461500049051543e-05\n",
            "Best model saved at epoch 58 with loss 2.6461500049051543e-05\n",
            "Epoch 59/200, Loss: 2.6581600236347217e-05\n",
            "Patience counter: 1\n",
            "Epoch 60/200, Loss: 2.6845710654434115e-05\n",
            "Patience counter: 2\n",
            "Epoch 61/200, Loss: 2.655543753032229e-05\n",
            "Patience counter: 3\n",
            "Early stopping triggered!\n",
            "I-JEPA pretraining complete!\n"
          ]
        }
      ],
      "source": [
        "# Pretrain the model using I-JEPA with validation loader\n",
        "pretrain_ijepa(context_encoder, target_encoder, predictor, train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ContextProcessor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ContextProcessor, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc = nn.Linear(128 * 56 * 56, 512)  # Adjust this based on input size after convolutions\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.adaptive_avg_pool2d(x, (56, 56))  # Adjust based on your architecture\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        return self.fc(x)  # Output shape should be (batch_size, 512)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Linear Probing - for downstream task(Classification)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0mtXL2_ctSU",
        "outputId": "9394a6c3-960f-44c4-92e6-a0772f67e438"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-15-1c85346dbe7a>:124: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100, Loss: 1.7926, Accuracy: 36.24%\n",
            "Validation Loss: 1.7442, Validation Accuracy: 38.29%\n",
            "Best model saved at epoch 1 with validation loss 1.7442\n",
            "Epoch 2/100, Loss: 1.6764, Accuracy: 40.79%\n",
            "Validation Loss: 1.6903, Validation Accuracy: 40.61%\n",
            "Best model saved at epoch 2 with validation loss 1.6903\n",
            "Epoch 3/100, Loss: 1.6577, Accuracy: 41.52%\n",
            "Validation Loss: 1.6670, Validation Accuracy: 40.66%\n",
            "Best model saved at epoch 3 with validation loss 1.6670\n",
            "Epoch 4/100, Loss: 1.6397, Accuracy: 42.37%\n",
            "Validation Loss: 1.7051, Validation Accuracy: 40.93%\n",
            "Epoch 5/100, Loss: 1.6273, Accuracy: 42.57%\n",
            "Validation Loss: 1.6900, Validation Accuracy: 40.87%\n",
            "Epoch 6/100, Loss: 1.6246, Accuracy: 42.65%\n",
            "Validation Loss: 1.6840, Validation Accuracy: 40.50%\n",
            "Epoch 7/100, Loss: 1.6150, Accuracy: 43.02%\n",
            "Validation Loss: 1.7064, Validation Accuracy: 41.50%\n",
            "Epoch 8/100, Loss: 1.6115, Accuracy: 43.31%\n",
            "Validation Loss: 1.6862, Validation Accuracy: 40.52%\n",
            "Epoch 9/100, Loss: 1.6119, Accuracy: 43.40%\n",
            "Validation Loss: 1.7282, Validation Accuracy: 39.84%\n",
            "Epoch 10/100, Loss: 1.6097, Accuracy: 43.78%\n",
            "Validation Loss: 1.6861, Validation Accuracy: 40.68%\n",
            "Epoch 11/100, Loss: 1.6063, Accuracy: 43.54%\n",
            "Validation Loss: 1.7087, Validation Accuracy: 39.49%\n",
            "Epoch 12/100, Loss: 1.6050, Accuracy: 43.35%\n",
            "Validation Loss: 1.7324, Validation Accuracy: 38.99%\n",
            "Epoch 13/100, Loss: 1.6066, Accuracy: 43.63%\n",
            "Validation Loss: 1.7767, Validation Accuracy: 39.94%\n",
            "Epoch 14/100, Loss: 1.6005, Accuracy: 43.89%\n",
            "Validation Loss: 1.7091, Validation Accuracy: 40.66%\n",
            "Epoch 15/100, Loss: 1.6023, Accuracy: 43.74%\n",
            "Validation Loss: 1.7575, Validation Accuracy: 39.89%\n",
            "Epoch 16/100, Loss: 1.6052, Accuracy: 43.81%\n",
            "Validation Loss: 1.6882, Validation Accuracy: 40.77%\n",
            "Epoch 17/100, Loss: 1.5978, Accuracy: 43.65%\n",
            "Validation Loss: 1.7210, Validation Accuracy: 40.90%\n",
            "Epoch 18/100, Loss: 1.5974, Accuracy: 43.65%\n",
            "Validation Loss: 1.7716, Validation Accuracy: 39.25%\n",
            "Epoch 19/100, Loss: 1.5984, Accuracy: 43.84%\n",
            "Validation Loss: 1.6765, Validation Accuracy: 41.62%\n",
            "Epoch 20/100, Loss: 1.5926, Accuracy: 44.04%\n",
            "Validation Loss: 1.6694, Validation Accuracy: 41.82%\n",
            "Epoch 21/100, Loss: 1.5977, Accuracy: 44.03%\n",
            "Validation Loss: 1.7043, Validation Accuracy: 40.83%\n",
            "Epoch 22/100, Loss: 1.5972, Accuracy: 43.96%\n",
            "Validation Loss: 1.6930, Validation Accuracy: 40.21%\n",
            "Epoch 23/100, Loss: 1.5994, Accuracy: 43.94%\n",
            "Validation Loss: 1.6670, Validation Accuracy: 41.43%\n",
            "Best model saved at epoch 23 with validation loss 1.6670\n",
            "Epoch 24/100, Loss: 1.5988, Accuracy: 43.91%\n",
            "Validation Loss: 1.7028, Validation Accuracy: 40.34%\n",
            "Epoch 25/100, Loss: 1.5967, Accuracy: 43.97%\n",
            "Validation Loss: 1.6996, Validation Accuracy: 39.30%\n",
            "Epoch 26/100, Loss: 1.5923, Accuracy: 44.28%\n",
            "Validation Loss: 1.7181, Validation Accuracy: 41.36%\n",
            "Epoch 27/100, Loss: 1.5970, Accuracy: 44.08%\n",
            "Validation Loss: 1.6573, Validation Accuracy: 42.02%\n",
            "Best model saved at epoch 27 with validation loss 1.6573\n",
            "Epoch 28/100, Loss: 1.6000, Accuracy: 44.07%\n",
            "Validation Loss: 1.6593, Validation Accuracy: 41.98%\n",
            "Epoch 29/100, Loss: 1.5914, Accuracy: 44.28%\n",
            "Validation Loss: 1.7048, Validation Accuracy: 41.15%\n",
            "Epoch 30/100, Loss: 1.5950, Accuracy: 44.13%\n",
            "Validation Loss: 1.7598, Validation Accuracy: 39.42%\n",
            "Epoch 31/100, Loss: 1.5906, Accuracy: 44.12%\n",
            "Validation Loss: 1.7891, Validation Accuracy: 38.21%\n",
            "Epoch 32/100, Loss: 1.5984, Accuracy: 43.97%\n",
            "Validation Loss: 1.7134, Validation Accuracy: 40.04%\n",
            "Epoch 33/100, Loss: 1.5957, Accuracy: 44.20%\n",
            "Validation Loss: 1.7334, Validation Accuracy: 40.39%\n",
            "Epoch 34/100, Loss: 1.5962, Accuracy: 44.02%\n",
            "Validation Loss: 1.6842, Validation Accuracy: 41.15%\n",
            "Epoch 35/100, Loss: 1.5938, Accuracy: 44.21%\n",
            "Validation Loss: 1.6800, Validation Accuracy: 41.35%\n",
            "Epoch 36/100, Loss: 1.5965, Accuracy: 44.00%\n",
            "Validation Loss: 1.7765, Validation Accuracy: 39.47%\n",
            "Epoch 37/100, Loss: 1.6005, Accuracy: 44.06%\n",
            "Validation Loss: 1.6783, Validation Accuracy: 41.23%\n",
            "Epoch 38/100, Loss: 1.5910, Accuracy: 44.03%\n",
            "Validation Loss: 1.6970, Validation Accuracy: 41.06%\n",
            "Epoch 39/100, Loss: 1.5964, Accuracy: 43.92%\n",
            "Validation Loss: 1.6860, Validation Accuracy: 41.73%\n",
            "Epoch 40/100, Loss: 1.5876, Accuracy: 44.47%\n",
            "Validation Loss: 1.6972, Validation Accuracy: 40.64%\n",
            "Epoch 41/100, Loss: 1.5951, Accuracy: 44.30%\n",
            "Validation Loss: 1.7704, Validation Accuracy: 39.06%\n",
            "Epoch 42/100, Loss: 1.5964, Accuracy: 44.19%\n",
            "Validation Loss: 1.7089, Validation Accuracy: 41.70%\n",
            "Epoch 43/100, Loss: 1.5968, Accuracy: 43.95%\n",
            "Validation Loss: 1.7307, Validation Accuracy: 40.34%\n",
            "Epoch 44/100, Loss: 1.5985, Accuracy: 44.07%\n",
            "Validation Loss: 1.6939, Validation Accuracy: 40.09%\n",
            "Epoch 45/100, Loss: 1.6010, Accuracy: 43.99%\n",
            "Validation Loss: 1.7062, Validation Accuracy: 41.10%\n",
            "Epoch 46/100, Loss: 1.5965, Accuracy: 44.02%\n",
            "Validation Loss: 1.8630, Validation Accuracy: 36.46%\n",
            "Epoch 47/100, Loss: 1.5933, Accuracy: 44.16%\n",
            "Validation Loss: 1.7619, Validation Accuracy: 40.03%\n",
            "Epoch 48/100, Loss: 1.5950, Accuracy: 43.94%\n",
            "Validation Loss: 1.6756, Validation Accuracy: 41.65%\n",
            "Epoch 49/100, Loss: 1.5967, Accuracy: 44.14%\n",
            "Validation Loss: 1.6850, Validation Accuracy: 41.20%\n",
            "Epoch 50/100, Loss: 1.5963, Accuracy: 44.03%\n",
            "Validation Loss: 1.6931, Validation Accuracy: 40.59%\n",
            "Epoch 51/100, Loss: 1.5928, Accuracy: 44.19%\n",
            "Validation Loss: 1.7485, Validation Accuracy: 39.56%\n",
            "Epoch 52/100, Loss: 1.5954, Accuracy: 43.96%\n",
            "Validation Loss: 1.7317, Validation Accuracy: 39.98%\n",
            "Epoch 53/100, Loss: 1.5910, Accuracy: 44.34%\n",
            "Validation Loss: 1.6903, Validation Accuracy: 41.97%\n",
            "Epoch 54/100, Loss: 1.6030, Accuracy: 43.96%\n",
            "Validation Loss: 1.7227, Validation Accuracy: 40.51%\n",
            "Epoch 55/100, Loss: 1.5991, Accuracy: 44.10%\n",
            "Validation Loss: 1.7084, Validation Accuracy: 41.21%\n",
            "Epoch 56/100, Loss: 1.5919, Accuracy: 44.18%\n",
            "Validation Loss: 1.7443, Validation Accuracy: 39.51%\n",
            "Epoch 57/100, Loss: 1.5972, Accuracy: 44.10%\n",
            "Validation Loss: 1.6901, Validation Accuracy: 40.87%\n",
            "Epoch 58/100, Loss: 1.5948, Accuracy: 44.06%\n",
            "Validation Loss: 1.6915, Validation Accuracy: 40.59%\n",
            "Epoch 59/100, Loss: 1.5915, Accuracy: 44.17%\n",
            "Validation Loss: 1.7411, Validation Accuracy: 39.69%\n",
            "Epoch 60/100, Loss: 1.5940, Accuracy: 44.24%\n",
            "Validation Loss: 1.7062, Validation Accuracy: 40.86%\n",
            "Epoch 61/100, Loss: 1.5991, Accuracy: 43.81%\n",
            "Validation Loss: 1.7014, Validation Accuracy: 40.93%\n",
            "Epoch 62/100, Loss: 1.5952, Accuracy: 43.97%\n",
            "Validation Loss: 1.7663, Validation Accuracy: 38.62%\n",
            "Epoch 63/100, Loss: 1.5954, Accuracy: 44.41%\n",
            "Validation Loss: 1.7184, Validation Accuracy: 41.26%\n",
            "Epoch 64/100, Loss: 1.5884, Accuracy: 44.21%\n",
            "Validation Loss: 1.6934, Validation Accuracy: 40.39%\n",
            "Epoch 65/100, Loss: 1.5961, Accuracy: 44.16%\n",
            "Validation Loss: 1.6856, Validation Accuracy: 40.50%\n",
            "Epoch 66/100, Loss: 1.5894, Accuracy: 44.16%\n",
            "Validation Loss: 1.6819, Validation Accuracy: 41.66%\n",
            "Epoch 67/100, Loss: 1.5987, Accuracy: 44.04%\n",
            "Validation Loss: 1.7659, Validation Accuracy: 39.10%\n",
            "Epoch 68/100, Loss: 1.5932, Accuracy: 44.32%\n",
            "Validation Loss: 1.7124, Validation Accuracy: 40.58%\n",
            "Epoch 69/100, Loss: 1.5956, Accuracy: 44.20%\n",
            "Validation Loss: 1.7020, Validation Accuracy: 40.46%\n",
            "Epoch 70/100, Loss: 1.5931, Accuracy: 44.08%\n",
            "Validation Loss: 1.7216, Validation Accuracy: 40.04%\n",
            "Epoch 71/100, Loss: 1.5937, Accuracy: 43.97%\n",
            "Validation Loss: 1.7127, Validation Accuracy: 40.79%\n",
            "Epoch 72/100, Loss: 1.5945, Accuracy: 44.34%\n",
            "Validation Loss: 1.6903, Validation Accuracy: 41.88%\n",
            "Epoch 73/100, Loss: 1.5993, Accuracy: 44.03%\n"
          ]
        }
      ],
      "source": [
        "class LinearProbingClassifier(nn.Module):\n",
        "    def __init__(self, predictor, num_classes=10):\n",
        "        super(LinearProbingClassifier, self).__init__()\n",
        "        self.context_processor = ContextProcessor()  # Initialize the context processor\n",
        "        self.predictor = predictor\n",
        "        self.fc = None\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def forward(self, context):\n",
        "        processed_context = self.context_processor(context)  # Process the context\n",
        "        pred_target_rep = self.predictor(processed_context)\n",
        "\n",
        "        # print(f\"Predicted representation shape: {pred_target_rep.shape}\")  # Debug output shape\n",
        "\n",
        "        if self.fc is None:\n",
        "            self.fc = nn.Linear(pred_target_rep.size(1), self.num_classes).to(context.device)\n",
        "\n",
        "        return self.fc(pred_target_rep)\n",
        "\n",
        "\n",
        "# Ensure to initialize the model and train it properly\n",
        "linear_probing_model = LinearProbingClassifier(predictor).to(device)\n",
        "\n",
        "\n",
        "def train_linear_probing(model, train_loader, val_loader, epochs=100, lr=0.1, patience=5):\n",
        "    model.train()\n",
        "    for images, _ in train_loader:\n",
        "        images = images.to(device)\n",
        "        context, _ = create_context_target_blocks(images)\n",
        "        context = context.to(device)\n",
        "        _ = model(context)  # Ensure fc layer is initialized during forward pass\n",
        "        break  # Run this only for the first batch to initialize the fc layer\n",
        "\n",
        "    optimizer = optim.Adam(model.fc.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_model_path = '/content/drive/MyDrive/best_linear_probing_model_cnn_final.pth'\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Create context block only\n",
        "            context, _ = create_context_target_blocks(images)\n",
        "            context = context.to(device)\n",
        "\n",
        "            # Forward pass using the predictor\n",
        "            outputs = model(context)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        accuracy = 100 * correct / total\n",
        "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "        # Validation loss computation\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for val_images, val_labels in val_loader:\n",
        "                val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
        "\n",
        "                context, _ = create_context_target_blocks(val_images)\n",
        "                context = context.to(device)\n",
        "\n",
        "                val_outputs = model(context)\n",
        "                loss = criterion(val_outputs, val_labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, val_predicted = torch.max(val_outputs.data, 1)\n",
        "                val_total += val_labels.size(0)\n",
        "                val_correct += (val_predicted == val_labels).sum().item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_accuracy = 100 * val_correct / val_total\n",
        "        print(f'Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "        # Check for early stopping\n",
        "        if avg_val_loss < best_loss:\n",
        "            best_loss = avg_val_loss\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f\"Best model saved at epoch {epoch + 1} with loss {best_loss:.4f}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"Patience counter: {patience_counter}\")\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping triggered!\")\n",
        "            break\n",
        "\n",
        "    print(\"Linear probing training complete!\")\n",
        "\n",
        "# Load pre-trained weights for predictor\n",
        "predictor = Predictor().to(device)  # Initialize the predictor\n",
        "\n",
        "checkpoint_path = '/content/drive/MyDrive/best_ijepa_model_CNN_final.pth'\n",
        "checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'), weights_only=True)\n",
        "predictor.load_state_dict(checkpoint['predictor'])  # Load predictor weights\n",
        "\n",
        "# Train the linear probing model with the updated classifier\n",
        "train_linear_probing(linear_probing_model, train_loader, val_loader)\n",
        "\n",
        "print(\"Training process is complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XJFZ_YinVEi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
