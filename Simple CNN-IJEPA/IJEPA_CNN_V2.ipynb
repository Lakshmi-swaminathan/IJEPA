{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EU_1Uq568X1V"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "from google.colab import drive\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Snpvj-KY8eWU",
    "outputId": "a1c98324-8c36-429c-91a9-08b3a50dbd48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "f5ido0rGOt0w"
   },
   "outputs": [],
   "source": [
    "# Define paths to save models in Google Drive\n",
    "best_model_path_ijepa = '/content/drive/My Drive/best_ijepa_model_CNN.pth'\n",
    "best_model_path_linear = '/content/drive/My Drive/best_linear_probing_model_CNN.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wVCmN4n4OznR"
   },
   "outputs": [],
   "source": [
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=128):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        # Set the expected size after the last convolutional layer\n",
    "        self.fc_input_size = 128 * 1 * 1  # Output shape after conv layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.fc_input_size, 256),  # Intermediate layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, embed_dim)  # Final output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        # print(f\"Shape after conv layers: {x.shape}\")\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        output = self.fc(x)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "emlfj9aFO7tv"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ContextEncoder(CNNEncoder):\n",
    "    def __init__(self, embed_dim=128):\n",
    "        super(ContextEncoder, self).__init__(embed_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "CwMDmnRXO-d_"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TargetEncoder(CNNEncoder):\n",
    "    def __init__(self, embed_dim=128):\n",
    "        super(TargetEncoder, self).__init__(embed_dim)\n",
    "\n",
    "    def update_target_weights(self, context_encoder, decay=0.99):\n",
    "        for target_param, context_param in zip(self.parameters(), context_encoder.parameters()):\n",
    "            target_param.data = decay * target_param.data + (1.0 - decay) * context_param.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JBLsJVNuPBJ6"
   },
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    def __init__(self, input_dim=256, output_dim=128):  # Add output_dim parameter\n",
    "        super(Predictor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)  # Input dimension remains 256\n",
    "        self.fc2 = nn.Linear(256, output_dim)  # Change output dimension to match actual_target_rep size\n",
    "\n",
    "    def forward(self, context_rep, mask_token):\n",
    "        x = torch.cat((context_rep, mask_token), dim=1)  # Shape [64, 256]\n",
    "        x = F.relu(self.fc1(x))  # First fully connected layer\n",
    "        x = self.fc2(x)  # Second fully connected layer\n",
    "        return x\n",
    "\n",
    "# Define the device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Initialize the context encoder, target encoder, and predictor\n",
    "context_encoder = ContextEncoder(embed_dim=128).to(device)\n",
    "target_encoder = TargetEncoder(embed_dim=128).to(device)\n",
    "predictor = Predictor(input_dim=256).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jLKWKeCCPErv"
   },
   "outputs": [],
   "source": [
    "# L2 loss function\n",
    "class IJEPALoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IJEPALoss, self).__init__()\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, pred_target_rep, actual_target_rep):\n",
    "        return self.criterion(pred_target_rep, actual_target_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qAduzF6_PID6"
   },
   "outputs": [],
   "source": [
    "def create_context_target_blocks(image, mask_ratio=0.25):\n",
    "    B, C, H, W = image.shape\n",
    "    mask_size = int(H * mask_ratio)\n",
    "\n",
    "    # Select context and target regions\n",
    "    context = image[:, :, :H - mask_size, :W - mask_size]\n",
    "    target = image[:, :, mask_size:, mask_size:]\n",
    "\n",
    "    # print(f\"Created Context Shape: {context.shape}\")\n",
    "    # print(f\"Created Target Shape: {target.shape}\")\n",
    "\n",
    "    return context.to(device), target.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "FQGiVUcXPMXq"
   },
   "outputs": [],
   "source": [
    "# Pretraining with I-JEPA using EMA for the target encoder\n",
    "def pretrain_ijepa(context_encoder, target_encoder, predictor, train_loader, start_epoch=0, epochs=200, lr=0.001, patience=5, decay=0.99):\n",
    "    optimizer = optim.Adam(list(context_encoder.parameters()) + list(predictor.parameters()), lr=lr)\n",
    "    criterion = IJEPALoss()\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        print(f\"Starting Epoch {epoch + 1}/{epochs}\")\n",
    "        running_loss = 0.0\n",
    "        for images, _ in train_loader:\n",
    "            images = images.to(device)  # Ensure input images are on the correct device\n",
    "            # print(f\"Images Device: {images.device}\")\n",
    "\n",
    "            # Create context and target blocks\n",
    "            context, target = create_context_target_blocks(images)\n",
    "            #print(f\"Context Device: {context.device}, Target Device: {target.device}\")\n",
    "            try:\n",
    "                # Get representations from context and target encoders\n",
    "                context_rep = context_encoder(context)\n",
    "                # print(f\"Context rep shape: {context_rep.shape}\")\n",
    "\n",
    "                actual_target_rep = target_encoder(target)\n",
    "                # print(f\"Target rep shape: {actual_target_rep.shape}\")\n",
    "\n",
    "                # Use mask tokens for prediction\n",
    "                mask_token = torch.randn_like(context_rep).to(device)  # Ensure mask_token is the same shape as context_rep\n",
    "                # print(f\"Mask Token shape: {mask_token.shape}\")\n",
    "\n",
    "                # Ensure all tensors are on the same device before prediction\n",
    "                assert context_rep.device == mask_token.device, \"context_rep and mask_token are on different devices!\"\n",
    "\n",
    "                # Now pass both context_rep and mask_token to the predictor\n",
    "                pred_target_rep = predictor(context_rep, mask_token)\n",
    "                # print(f\"Predicted Target Representation Device: {pred_target_rep.device}\")  # Debugging print\n",
    "\n",
    "                # Check if both tensors for loss are on the same device\n",
    "                assert pred_target_rep.device == actual_target_rep.device, \"pred_target_rep and actual_target_rep are on different devices!\"\n",
    "\n",
    "                # Compute the loss\n",
    "                loss = criterion(pred_target_rep, actual_target_rep)\n",
    "\n",
    "                # Backpropagation and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "            except Exception as e:\n",
    "                print(f\"Error encountered: {e}\")\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "        # Update target encoder weights using EMA\n",
    "        target_encoder.update_target_weights(context_encoder, decay=decay)\n",
    "\n",
    "        # Early stopping and model saving logic\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'context_encoder': context_encoder.state_dict(),\n",
    "                'target_encoder': target_encoder.state_dict(),\n",
    "                'predictor': predictor.state_dict(),\n",
    "            }, best_model_path_ijepa)\n",
    "            print(f\"Best I-JEPA model saved at epoch {epoch + 1} with loss {best_loss:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered!\")\n",
    "                break\n",
    "\n",
    "    print(\"I-JEPA pretraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "egn2Wfa5PWMb"
   },
   "outputs": [],
   "source": [
    "# DataLoader for CIFAR-10 with normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # Normalization based on CIFAR-10 dataset stats\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "keXU6tB-PZV_",
    "outputId": "64027dd0-a408-4c9d-e8fb-b9bc2963b37b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:17<00:00, 9.95MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
     ]
    }
   ],
   "source": [
    "# Load the full CIFAR-10 dataset\n",
    "full_trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "oUkLvPQ3Pb7y"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into train, validation, and test sets\n",
    "train_size = len(full_trainset) - 10000  # 5000 for validation + 5000 for testing\n",
    "val_size = 5000\n",
    "test_size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "w-5amDPvPeqW"
   },
   "outputs": [],
   "source": [
    "trainset, valset, testset = random_split(full_trainset, [train_size, val_size, test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "wl46ApyIPhTC"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_loader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(valset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zIqRFwr7P4iO",
    "outputId": "f7f0456c-f783-4c6e-e240-c0eee863c31e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1/200\n",
      "Epoch 1/200, Loss: 0.0181\n",
      "Best I-JEPA model saved at epoch 1 with loss 0.0181\n",
      "Starting Epoch 2/200\n",
      "Epoch 2/200, Loss: 0.0140\n",
      "Best I-JEPA model saved at epoch 2 with loss 0.0140\n",
      "Starting Epoch 3/200\n",
      "Epoch 3/200, Loss: 0.0129\n",
      "Best I-JEPA model saved at epoch 3 with loss 0.0129\n",
      "Starting Epoch 4/200\n",
      "Epoch 4/200, Loss: 0.0120\n",
      "Best I-JEPA model saved at epoch 4 with loss 0.0120\n",
      "Starting Epoch 5/200\n",
      "Epoch 5/200, Loss: 0.0113\n",
      "Best I-JEPA model saved at epoch 5 with loss 0.0113\n",
      "Starting Epoch 6/200\n",
      "Epoch 6/200, Loss: 0.0107\n",
      "Best I-JEPA model saved at epoch 6 with loss 0.0107\n",
      "Starting Epoch 7/200\n",
      "Epoch 7/200, Loss: 0.0101\n",
      "Best I-JEPA model saved at epoch 7 with loss 0.0101\n",
      "Starting Epoch 8/200\n",
      "Epoch 8/200, Loss: 0.0096\n",
      "Best I-JEPA model saved at epoch 8 with loss 0.0096\n",
      "Starting Epoch 9/200\n",
      "Epoch 9/200, Loss: 0.0091\n",
      "Best I-JEPA model saved at epoch 9 with loss 0.0091\n",
      "Starting Epoch 10/200\n",
      "Epoch 10/200, Loss: 0.0087\n",
      "Best I-JEPA model saved at epoch 10 with loss 0.0087\n",
      "Starting Epoch 11/200\n",
      "Epoch 11/200, Loss: 0.0083\n",
      "Best I-JEPA model saved at epoch 11 with loss 0.0083\n",
      "Starting Epoch 12/200\n",
      "Epoch 12/200, Loss: 0.0079\n",
      "Best I-JEPA model saved at epoch 12 with loss 0.0079\n",
      "Starting Epoch 13/200\n",
      "Epoch 13/200, Loss: 0.0075\n",
      "Best I-JEPA model saved at epoch 13 with loss 0.0075\n",
      "Starting Epoch 14/200\n",
      "Epoch 14/200, Loss: 0.0072\n",
      "Best I-JEPA model saved at epoch 14 with loss 0.0072\n",
      "Starting Epoch 15/200\n",
      "Epoch 15/200, Loss: 0.0069\n",
      "Best I-JEPA model saved at epoch 15 with loss 0.0069\n",
      "Starting Epoch 16/200\n",
      "Epoch 16/200, Loss: 0.0065\n",
      "Best I-JEPA model saved at epoch 16 with loss 0.0065\n",
      "Starting Epoch 17/200\n",
      "Epoch 17/200, Loss: 0.0063\n",
      "Best I-JEPA model saved at epoch 17 with loss 0.0063\n",
      "Starting Epoch 18/200\n",
      "Epoch 18/200, Loss: 0.0060\n",
      "Best I-JEPA model saved at epoch 18 with loss 0.0060\n",
      "Starting Epoch 19/200\n",
      "Epoch 19/200, Loss: 0.0058\n",
      "Best I-JEPA model saved at epoch 19 with loss 0.0058\n",
      "Starting Epoch 20/200\n",
      "Epoch 20/200, Loss: 0.0056\n",
      "Best I-JEPA model saved at epoch 20 with loss 0.0056\n",
      "Starting Epoch 21/200\n",
      "Epoch 21/200, Loss: 0.0054\n",
      "Best I-JEPA model saved at epoch 21 with loss 0.0054\n",
      "Starting Epoch 22/200\n",
      "Epoch 22/200, Loss: 0.0052\n",
      "Best I-JEPA model saved at epoch 22 with loss 0.0052\n",
      "Starting Epoch 23/200\n",
      "Epoch 23/200, Loss: 0.0050\n",
      "Best I-JEPA model saved at epoch 23 with loss 0.0050\n",
      "Starting Epoch 24/200\n",
      "Epoch 24/200, Loss: 0.0049\n",
      "Best I-JEPA model saved at epoch 24 with loss 0.0049\n",
      "Starting Epoch 25/200\n",
      "Epoch 25/200, Loss: 0.0048\n",
      "Best I-JEPA model saved at epoch 25 with loss 0.0048\n",
      "Starting Epoch 26/200\n",
      "Epoch 26/200, Loss: 0.0047\n",
      "Best I-JEPA model saved at epoch 26 with loss 0.0047\n",
      "Starting Epoch 27/200\n",
      "Epoch 27/200, Loss: 0.0046\n",
      "Best I-JEPA model saved at epoch 27 with loss 0.0046\n",
      "Starting Epoch 28/200\n",
      "Epoch 28/200, Loss: 0.0045\n",
      "Best I-JEPA model saved at epoch 28 with loss 0.0045\n",
      "Starting Epoch 29/200\n",
      "Epoch 29/200, Loss: 0.0045\n",
      "Best I-JEPA model saved at epoch 29 with loss 0.0045\n",
      "Starting Epoch 30/200\n",
      "Epoch 30/200, Loss: 0.0044\n",
      "Best I-JEPA model saved at epoch 30 with loss 0.0044\n",
      "Starting Epoch 31/200\n",
      "Epoch 31/200, Loss: 0.0044\n",
      "Best I-JEPA model saved at epoch 31 with loss 0.0044\n",
      "Starting Epoch 32/200\n",
      "Epoch 32/200, Loss: 0.0044\n",
      "Best I-JEPA model saved at epoch 32 with loss 0.0044\n",
      "Starting Epoch 33/200\n",
      "Epoch 33/200, Loss: 0.0043\n",
      "Best I-JEPA model saved at epoch 33 with loss 0.0043\n",
      "Starting Epoch 34/200\n",
      "Epoch 34/200, Loss: 0.0043\n",
      "Starting Epoch 35/200\n",
      "Epoch 35/200, Loss: 0.0044\n",
      "Starting Epoch 36/200\n",
      "Epoch 36/200, Loss: 0.0044\n",
      "Starting Epoch 37/200\n",
      "Epoch 37/200, Loss: 0.0044\n",
      "Starting Epoch 38/200\n",
      "Epoch 38/200, Loss: 0.0044\n",
      "Early stopping triggered!\n",
      "I-JEPA pretraining complete!\n"
     ]
    }
   ],
   "source": [
    "# Pretrain the model using I-JEPA\n",
    "pretrain_ijepa(context_encoder, target_encoder, predictor, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cinfxDEyW3ip",
    "outputId": "6fc1be43-1fe5-4392-f308-06da65f51822"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-5882530dcaaa>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(best_model_path_ijepa, map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pre-trained model checkpoint\n",
    "checkpoint = torch.load(best_model_path_ijepa, map_location=device)\n",
    "\n",
    "# Load full state_dicts directly, without filtering 'fc' layers\n",
    "context_encoder.load_state_dict(checkpoint['context_encoder'], strict=False)\n",
    "target_encoder.load_state_dict(checkpoint['target_encoder'], strict=False)\n",
    "predictor.load_state_dict(checkpoint['predictor'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
