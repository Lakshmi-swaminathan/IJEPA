{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "X-vUClUuhfz0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU is not available\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2FEplR8hnR3",
        "outputId": "c1bcc66d-377e-4c24-dcea-dc419e97a139"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is not available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "# Transform: Resize to 224x224 and convert to Tensor\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to 224x224 for ResNet50\n",
        "    transforms.ToTensor(),  # Convert to Tensor\n",
        "])\n",
        "\n",
        "# Load CIFAR-100 training dataset\n",
        "cifar100_train = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Create a DataLoader for CIFAR-100 training data\n",
        "dataloader = DataLoader(cifar100_train, batch_size=512, shuffle=True, num_workers=4)\n",
        "\n",
        "# Explanation:\n",
        "# - batch_size=32: Loads 32 images per batch.\n",
        "# - shuffle=True: Shuffles the dataset at every epoch to improve training.\n",
        "# - num_workers=4: Uses 4 subprocesses to load the data in parallel (improves performance)."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3G4hntOkhpFC",
        "outputId": "d2441437-83b4-4aba-eafa-564d373e44f7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:04<00:00, 40.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super(Encoder, self).__init__()\n",
        "        # Load ResNet model and exclude the fully connected layer\n",
        "        resnet = resnet50(pretrained=pretrained)\n",
        "        resnet.avgpool = nn.Identity()\n",
        "        # Remove layers that reduce spatial dimensions too much\n",
        "        self.features = nn.Sequential(*list(resnet.children())[:-2])  # Use layers up to an earlier conv layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.features(x)\n",
        "        return features"
      ],
      "metadata": {
        "id": "0M476YhHhpxf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Predictor(nn.Module):\n",
        "    def __init__(self, input_dim=4096, hidden_dim=4096, output_dim=4096):\n",
        "        super(Predictor, self).__init__()\n",
        "\n",
        "        # Linear layers to predict target features from context features\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)  # Input to hidden layer\n",
        "        self.relu = nn.ReLU()                       # Non-linearity\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim) # Hidden to output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the predictor\n",
        "        x = self.fc1(x)         # Input -> Hidden layer\n",
        "        x = self.relu(x)        # Non-linearity\n",
        "        x = self.fc2(x)         # Hidden layer -> Output\n",
        "        return x"
      ],
      "metadata": {
        "id": "skQLnCcFhuEI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3, verbose=False):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.best_loss = np.inf\n",
        "        self.epochs_without_improvement = 0\n",
        "\n",
        "    def step(self, current_loss):\n",
        "        if current_loss < self.best_loss:\n",
        "            self.best_loss = current_loss\n",
        "            self.epochs_without_improvement = 0\n",
        "            if self.verbose:\n",
        "                print(f\"Validation loss improved to {current_loss:.6f}.\")\n",
        "        else:\n",
        "            self.epochs_without_improvement += 1\n",
        "            if self.verbose:\n",
        "                print(f\"No improvement in validation loss. Count: {self.epochs_without_improvement}/{self.patience}\")\n",
        "\n",
        "        return self.epochs_without_improvement >= self.patience\n"
      ],
      "metadata": {
        "id": "-xBK7OQ523lh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EMA:\n",
        "    def __init__(self, model, decay=0.99):\n",
        "        \"\"\"\n",
        "        Initialize EMA with a given model and decay rate.\n",
        "\n",
        "        Args:\n",
        "        - model: The model to track with EMA (usually the target encoder).\n",
        "        - decay: The decay rate for updating EMA (default: 0.99).\n",
        "        \"\"\"\n",
        "        self.model = model  # The target encoder\n",
        "        self.decay = decay  # EMA decay factor (typically close to 1, e.g., 0.99)\n",
        "        self.shadow = {}    # Stores the moving average of the parameters\n",
        "        self.backup = {}    # Temporary backup of model parameters during shadow application\n",
        "\n",
        "    def register(self):\n",
        "        \"\"\"Initialize the shadow weights with the original model weights.\"\"\"\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                self.shadow[name] = param.data.clone()\n",
        "\n",
        "    def update(self):\n",
        "        \"\"\"Update the shadow weights using the current model parameters with EMA.\"\"\"\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                # EMA update: shadow[name] = decay * shadow[name] + (1 - decay) * param\n",
        "                self.shadow[name] = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n",
        "\n",
        "    def apply_shadow(self):\n",
        "        \"\"\"Replace the model parameters with the EMA weights (shadow weights).\"\"\"\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                # Backup current parameters\n",
        "                self.backup[name] = param.data.clone()\n",
        "                # Replace parameters with EMA weights\n",
        "                param.data = self.shadow[name]\n",
        "\n",
        "    def restore(self):\n",
        "        \"\"\"Restore the original model parameters after using the shadow weights.\"\"\"\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                # Restore from backup\n",
        "                param.data = self.backup[name]\n",
        "        # Clear the backup\n",
        "        self.backup = {}"
      ],
      "metadata": {
        "id": "KokgS9jehyhn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# Positional Embedding Layer for the patches\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, grid_size, embed_dim):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "        self.grid_size = grid_size  # E.g., 7 for a 7x7 grid (49 patches total)\n",
        "        self.embed_dim = embed_dim  # Embedding dimension (e.g., 512 to match ResNet50 output)\n",
        "\n",
        "        # Learnable positional embeddings\n",
        "        self.positional_embeddings = nn.Parameter(torch.randn(grid_size * grid_size, embed_dim))\n",
        "\n",
        "    def forward(self):\n",
        "        return self.positional_embeddings  # Shape: (grid_size * grid_size, embed_dim)"
      ],
      "metadata": {
        "id": "_ab8ixt8h5If"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_blocks_without_positional_embeddings(image, context_encoder, M=4, N=49, context_ratio=0.85, device=torch.device(\"cpu\")):\n",
        "    \"\"\"\n",
        "    Generate context and target blocks without positional embeddings.\n",
        "    \"\"\"\n",
        "    _, H, W = image.shape  # Assume image in [C, H, W] format, with C=3\n",
        "    grid_size = int(np.sqrt(N))\n",
        "    patch_h, patch_w = H // grid_size, W // grid_size\n",
        "\n",
        "    # Step 1: Split the image into patches\n",
        "    patches = []\n",
        "    for i in range(grid_size):\n",
        "        for j in range(grid_size):\n",
        "            patch = image[:, i*patch_h:(i+1)*patch_h, j*patch_w:(j+1)*patch_w]  # (3, patch_h, patch_w)\n",
        "            patches.append(patch.to(device))\n",
        "\n",
        "    # Select context and target patches\n",
        "    num_context_patches = int(context_ratio * N)\n",
        "    context_indices = np.random.choice(range(N), size=num_context_patches, replace=False)\n",
        "    target_indices = [i for i in range(N) if i not in context_indices]\n",
        "\n",
        "    # Stack patches for encoder compatibility\n",
        "    context_patches = torch.stack([patches[i] for i in context_indices], dim=0)  # Shape: [num_context_patches, 3, patch_h, patch_w]\n",
        "    target_patches = torch.stack([patches[i] for i in target_indices[:M]], dim=0)  # Shape: [M, 3, patch_h, patch_w]\n",
        "\n",
        "    return context_patches, target_patches"
      ],
      "metadata": {
        "id": "z_aEoxt8h84u"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Initialize encoders\n",
        "context_encoder = Encoder().to(device)\n",
        "target_encoder = Encoder().to(device)  # Target encoder uses EMA of context encoder's weights\n",
        "predictor = Predictor().to(device)\n",
        "\n",
        "# Initialize EMA\n",
        "ema = EMA(target_encoder, decay=0.99)\n",
        "ema.register()  # Register initial weights\n",
        "\n",
        "# Optimizer for context encoder and predictor\n",
        "optimizer = optim.Adam(list(context_encoder.parameters()) + list(predictor.parameters()), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "# Early stopping\n",
        "early_stopping = EarlyStopping(patience=3, verbose=True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGIEzw8dh_Bc",
        "outputId": "efb6eb5b-5c49-4af7-a1ba-46f4093c4066"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 103MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm  # Progress bar\n",
        "\n",
        "# Training loop with progress tracking\n",
        "for epoch in range(100):\n",
        "    epoch_loss = 0.0\n",
        "    print(f\"Epoch [{epoch + 1}/10]\")\n",
        "\n",
        "    # Add a progress bar to track batch progress within each epoch\n",
        "    for batch_idx, (images, _) in enumerate(tqdm(dataloader, desc=\"Training\", leave=False)):\n",
        "        images = images.to(device)\n",
        "\n",
        "        # Generate context and target blocks without positional embeddings\n",
        "        context_patches, target_patches = generate_blocks_without_positional_embeddings(\n",
        "            images[0], context_encoder=context_encoder, M=4, N=49, device=device\n",
        "        )\n",
        "\n",
        "        # Forward pass through context encoder\n",
        "        context_features_all = torch.stack([context_encoder(patch.unsqueeze(0)).squeeze(0) for patch in context_patches])\n",
        "\n",
        "        # Select only the first M context features to match the number of target features\n",
        "        context_features = context_features_all[:len(target_patches)]\n",
        "        context_features = context_features.view(context_features.size(0), -1)\n",
        "\n",
        "        # Apply shadow (EMA weights) to the target encoder for this step\n",
        "        ema.apply_shadow()  # Use EMA weights for target encoder\n",
        "        target_features = torch.stack([target_encoder(patch.unsqueeze(0)).squeeze(0) for patch in target_patches])\n",
        "        ema.restore()       # Restore original weights after prediction\n",
        "\n",
        "        # Predict target features using predictor\n",
        "        predicted_target_features = predictor(context_features)\n",
        "\n",
        "        # Compute loss (MSE between predicted and actual target features)\n",
        "        loss = nn.MSELoss()(predicted_target_features, target_features.view(target_features.size(0), -1))\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Backpropagation and update weights\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update EMA for target encoder\n",
        "        ema.update()\n",
        "\n",
        "        # Print batch loss periodically (every 10 batches)\n",
        "        # if batch_idx % 10 == 0:\n",
        "        #     print(f\"Epoch [{epoch + 1}/10], Batch [{batch_idx}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Print average loss for the epoch\n",
        "    avg_epoch_loss = epoch_loss / len(dataloader)\n",
        "    print(f\"Epoch [{epoch + 1}/10] Average Loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "    if early_stopping.step(avg_epoch_loss):\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break  # Exit the training loop if early stopping is triggered\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        torch.save(context_encoder.state_dict(), f\"context_encoder_epoch_{epoch + 1}.pth\")\n",
        "        torch.save(target_encoder.state_dict(), f\"target_encoder_epoch_{epoch + 1}.pth\")\n",
        "        torch.save(predictor.state_dict(), f\"predictor_epoch_{epoch + 1}.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MTcSx2kBiA3b",
        "outputId": "be20cb56-ea20-4fc2-bd17-0d20f570b2c3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] Average Loss: 0.0037\n",
            "Validation loss improved to 0.003717.\n",
            "Epoch [2/10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10] Average Loss: 0.0035\n",
            "Validation loss improved to 0.003522.\n",
            "Epoch [3/10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/10] Average Loss: 0.0036\n",
            "No improvement in validation loss. Count: 1/5\n",
            "Epoch [4/10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/10] Average Loss: 0.0035\n",
            "Validation loss improved to 0.003467.\n",
            "Epoch [5/10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/10] Average Loss: 0.0035\n",
            "No improvement in validation loss. Count: 1/5\n",
            "Epoch [6/10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/10] Average Loss: 0.0035\n",
            "No improvement in validation loss. Count: 2/5\n",
            "Epoch [7/10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/10] Average Loss: 0.0035\n",
            "Validation loss improved to 0.003464.\n",
            "Epoch [8/10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/10] Average Loss: 0.0035\n",
            "No improvement in validation loss. Count: 1/5\n",
            "Epoch [9/10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/10] Average Loss: 0.0034\n",
            "Validation loss improved to 0.003445.\n",
            "Epoch [10/10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/10] Average Loss: 0.0035\n",
            "No improvement in validation loss. Count: 1/5\n",
            "Epoch [11/10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/10] Average Loss: 0.0035\n",
            "No improvement in validation loss. Count: 2/5\n",
            "Epoch [12/10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [12/10] Average Loss: 0.0035\n",
            "No improvement in validation loss. Count: 3/5\n",
            "Epoch [13/10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13/10] Average Loss: 0.0034\n",
            "Validation loss improved to 0.003431.\n",
            "Epoch [14/10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/10] Average Loss: 0.0035\n",
            "No improvement in validation loss. Count: 1/5\n",
            "Epoch [15/10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-b808568262d8>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Backpropagation and update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}